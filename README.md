# PapersReview by E-Deep

## 일시 
: 매주 목요일 저녁 9시 30분

## 목적
: 꾸준한 딥러닝 공부 / 최신 기술을 학습 / 딥러닝계 친목

## 방법
: 딥러닝에 관심있는 사람들이 모여 매주 동일한 논문을 읽고 정해진 한 스터디원이 발표(zoom사용)

## 논문 선정 
: 발표자가 일주일 전 공지(Computer vision, NLP논문 리스트 + 각자 원하는 논문도 언제나 환영)

## 규칙
1. 발표자의 발표 자료 및 미발표자의 리뷰는 발표 전날인 수요일까지 업로드(미발표자의 리뷰는 발표자가 취합 후 삭제예정)
2. 피치못한 사정으로 참여가 불가능 할 경우 월요일 전에 통보(발표자 외)
3. 피치못한 사정으로 발표가 불가능 할 경우 일요일 전에 통보(발표자) → 다른 순서가 발표를 넘겨받음
4. 1,2번에 대한 벌금 : 발표자불참시 5000원, 발표불참 2000원 (보증금 10000원)
5. 발표 후 각자 짧게라도 리뷰를 적은 내용을 발표자가 추합(발표 자료 포함)해서 다음날인 금요일까지 깃허브에 올림(ex. 2020-12-30-BERT.md 형식)

## 발표순서
전세희 -> 김연수 -> 백서인 -> 구재원 -> 신한이 로 무한회전 

## 그 외

- 질문과 의견 제시는 자유롭게 🧐
- 딥러닝계의 우리들의 생태계를 만들어 보아요 🤪
- 코로나가 끝나면 오프라인 스터디도 고려🤓
- 발표 외부에 공개 또는 외부인 청강 / 발표 영상 녹화도 고려중
- 논문 리스트는 12/31에 스터디리더 전세희가 업로드 예정
- 논문 소스코드도 같이 업로드하고 돌려보는 것 권장(선택사항)


그 외 의견 언제든지 공유해주세요😁

## 발표 논문 리스트

### Image: Segmentation / Object Detection

1. You only look once: Unified, real-time object detection (2016), J. Redmon et al. [pdf]
2. Fully convolutional networks for semantic segmentation (2015), J. Long et al. [pdf]
3. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015), S. Ren et al. [pdf]
4. Fast R-CNN (2015), R. Girshick [pdf]

### Convolutional Neural Network Models

1. Inception-v4, inception-resnet and the impact of residual connections on learning (2016), C. Szegedy et al. [pdf]
2. Identity Mappings in Deep Residual Networks (2016), K. He et al. [pdf]
3. Deep residual learning for image recognition (2016), K. He et al. [pdf]

### Optimization / Training Techniques

1. Dropout: A simple way to prevent neural networks from overfitting (2014), N. Srivastava et al. [pdf]
2. Adam: A method for stochastic optimization (2014), D. Kingma and J. Ba [pdf]

### Natural Language Proecessing

- Word Embeddings
1. NPLM : A Neural Probabilistic Language Model, 2003 (https://papers.nips.cc/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)
2. Word2Vec : Efficient Estimation of Word Representations in Vector Space (https://arxiv.org/abs/1301.3781)
3. FastText : Enriching Word Vectors with Subword Information (https://arxiv.org/abs/1607.04606v2)
4. GloVe : GloVe: Global Vectors for Word Representation (https://www.aclweb.org/anthology/D14-1162.pdf)
5. Swivel : Swivel: Improving Embeddings by Noticing What's Missing (https://arxiv.org/abs/1602.02215)

- Sentence Embeddings
1. Doc2Vec : Distributed Representations of Sentences and Documents (https://arxiv.org/abs/1405.4053)
2. ELMo : Deep contextualized word representations (https://arxiv.org/abs/1802.05365)
3. Transformer : Attention Is All You Need (https://arxiv.org/abs/1706.03762)
4. BERT : BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (https://arxiv.org/abs/1810.04805)

### Generative Adversarial Networks(GAN)
1. GAN : Generative Adversarial Networks (https://arxiv.org/abs/1406.2661)
2. DCGAN : Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (https://arxiv.org/abs/1511.06434)
3. CycleGAN : Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (https://arxiv.org/abs/1703.10593)
4. StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks (https://arxiv.org/abs/1812.04948)
5. StarGAN : StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation (https://arxiv.org/abs/1711.09020)


## 참고할만한 사이트들

- https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap
- https://deeplearn.org/
- https://paperswithcode.com/greatest
- https://www.notion.so/c3b3474d18ef4304b23ea360367a5137?v=5d763ad5773f44eb950f49de7d7671bd

