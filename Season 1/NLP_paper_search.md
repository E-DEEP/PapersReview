### Natural Language Proecessing

#### Machine Learning
1. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier, Marco Tulio Ribeiro, KDD 2016. (https://arxiv.org/abs/1602.04938)

#### Nueral Models
1. Deep contextualized word representations, Matthew E. Peters, 2018. (https://arxiv.org/abs/1802.05365)
2. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, 2018. (https://arxiv.org/abs/1810.04805)
3. OpenAI GPT-1 - Improving Language Understanding by Generative Pre-Training, Alec Radford, 2018 (https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
4. XLNet: Generalized Autoregressive Pretraining for Language Understanding, Zhilin Yang, 2019 (https://arxiv.org/abs/1906.08237)  
5. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, Zhenzhong Lan, 2020(https://arxiv.org/abs/1909.11942)

#### Clustering & Word/Sentence Embeddings
1. Word2Vec : Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov, 2013 (https://arxiv.org/abs/1301.3781))
2. GloVe: Global Vectors for Word Representation, Jeffrey Pennington, 2014 (https://www.aclweb.org/anthology/D14-1162/)
3. Universal Sentence Encoder, Daniel Cer et al, 2018 (https://arxiv.org/abs/1803.11175)

#### Language Modeling
1. OpenAI GPT-2: Language Models are Unsupervised Multitask Learners, Alec Radford, 2019 (http://www.persagen.com/files/misc/radford2019language.pdf)
2. OpenAI GPT-3: Language Models are Few-Shot Learners, Alec Radford et al, 2020 ([https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165))

#### Machine Translation

1. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, Kyunghyun Cho, 2014 (https://arxiv.org/abs/1409.1259)
2. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, Yonghui Wu, 2016 (https://arxiv.org/abs/1609.08144)
3. Convolutional Sequence to Sequence Learning, Jonas Gehring, 2017. (https://arxiv.org/abs/1705.03122)

#### Summarization
1. A Neural Attention Model for Abstractive Sentence Summarization, Alexander M. Rush, 2015 (https://arxiv.org/abs/ a1509.00685)

#### Domain Adaptation
1. Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks, Suchin Gururangan, 2020  (https://arxiv.org/abs/2004.10964)
