# E-DEEP SEASON 2

### 1. 새로운 분야 추가 제안
(추천시스템, speech, 강화학습, 메타러닝 등등)
자신이 제안하는 분야에 대한 소개와 요즘 트렌드 등을 목요일 전까지 깃헙에 써주세요. 이번주 목요일 모임에서 소개하고 이야기를 나눠볼 예정입니다.

### 2. 코드 리뷰

- 저번에 말한 대로 한 달은 논문 리뷰, 한 달은 코드 리뷰가 진행될 예정입니다. 
  따라서 리뷰할 논문과 코드를 선정해야하는데 우선 추가할 분야는 명확하게 정해지지 않았으니 비전과 NLP분야 논문과 코드 리스트를 각자 5개씩 리스트업해서 올리는 것으로 하겠습니다.
- 비전 논문 5/ 코드 5 
- 자연어 논문 5/ 코드5
- 목요일 모임 이후 새롭게 선정된 분야에 대해서도 리스트업을 진행할 것입니다.
(우선 다음주 논문리뷰는 비전과 자연어에서 진행)
  
  
### 3. 
  
  - 4월 한 달간은 탐색의 시간을 가지려고 합니다.
    발표 순서 :  - 4/8 : 한지수, 전세희X
                - 4/15 : 구재원X, 이유정
                - 4/22 : 구재원, 류정현
                - 4/29 : 신한이, 김연수
                - 5/6 : 류정현, 전세희
                
  - 각자 원하는 분야 논문 발표(1주에 2명)
  - 논문 정하신 분은 여기에 써주세요!
  (EX. 홍길동 4/1 발표 - CNN (논문링크))
    - 한지수 4/8 발표 - Mask R-CNN [link](https://arxiv.org/pdf/1703.06870.pdf)
    - 이유정 4/15 발표 -  MAML [link](https://arxiv.org/pdf/1703.03400.pdf)
    - 구재원 4/22 발표 -  CLIP [link](https://arxiv.org/pdf/2103.00020.pdf)
    - 백서인 4/22 발표 - StyleGAN [link](https://arxiv.org/pdf/1812.04948.pdf)
    - 김연수 4/29 발표 - Everybody Dance Now [link](https://arxiv.org/abs/1808.07371)
    - 신한이 4/29 발표 - StyleCLIP [link](https://arxiv.org/pdf/2103.17249.pdf)
    - 류정현 5/6 발표  - RepVGG: Making VGG-style ConvNets Great Again [link](https://arxiv.org/abs/2101.03697)
    - 전세희 5/6 발표 - Variational Autoencoders for Collaborative Filtering [link](https://arxiv.org/abs/1802.05814)
  - 안정하신 분은 4/4 23:59:59까지 올리기
  - 다른 의견 언제나 환영


### 4. [21/05/06] 분야 확정 : 비전/자연어/멀티모달/메타러닝

5/11까지 비전/자연어/멀티모달/메타러닝 분야 별 최소 1개이상의 논문을 리스트업해주세요!(5/12에 투표로 분야별 4개씩 선정)

#### 비전

    - 한지수 : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT, facebook)
    - 이유정 : 
    - 구재원 : Emerging Properties in Self-Supervised Vision Transformers (Facebook)
    - 백서인 : Analyzing and Improving the Image Quality of StyleGAN (NVIDIA)
    - 김연수 : Few-shot Video-to-Video Synthesis(Few-shot Vid2Vid, NVIDIA)
    - 신한이 : Generative Pretraining from Pixels
    - 류정현 : EfficientDet: Scalable and Efficient Object Detection (Google Research)
    - 전세희 : Is Space-Time Attention All You Need for Video Understanding? & 
    Generative Pretraining from Pixels(ImageGPT)

#### 자연어


    - 한지수: Neural Architecture Search with Reinforcement Learning (Google)
    - 이유정 : Self-supervised learning with swin transformers (Microsoft)
    - 구재원 : Learning to Perturb Word Embeddings for Out-of-distribution QA (KAIST, ACL 2021)
    - 백서인 : XLNet: Generalized Autoregressive Pretraining for Language Understanding (Google)
    - 김연수 : PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization (ICML 2020)
    - 신한이 : DeText: A Deep Text Ranking Framework with BERT
    - 류정현 : ALBERT; A Lite BERT for Self-supervised Learning of Language Representations
    - 전세희 : XLNet: Generalized Autoregressive Pretraining for Language Understanding 
    - 

#### 멀티모달


    - 한지수: Self-Supervised MultiModal Versatile Networks (Google)
    - 이유정 :Zeroshot text-to-image generation (OpenAI)
    - 구재원: DALL-E: Zero-Shot Text-to-Image Generation (OpenAI)
    - 백서인: Multimodal Transformer for Unaligned Multimodal Language Sequences (Carnegie Mellon University)
    - 김연수 : Multi-Modal Dense Video Captioning 
    - 신한이 : What Makes Training Multi-modal Classification Networks Hard?
    - 류정현 : VideoBERT: A Joint Model for Video and Language Representation Learning (Google Research)
    - 전세희 : DALL-E: Zero-Shot Text-to-Image Generation
    

#### 메타러닝


    - 한지수: Matching Networks for One Shot Learning(Google)
    - 이유정 : MetaAlign: Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation(CVPR 2021)
    - 구재원 : Learning to Learn Words from Visual Scenes (Columbia University, ECCV 2020)
    - 백서인 : Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks 
    - 김연수 : Online Meta-Learning (ICML, 2019)
    - 신한이 : Generalizing from a Few Examples: A survey on Few-Shot Learning
    - 류정현 
    - 전세희 : Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning


