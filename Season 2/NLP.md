# Natural Language Processing 논문 리스트(5개)

## 구재원
1. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations [paper](https://arxiv.org/pdf/1909.11942.pdf)
2. GPT-3: Language Models are Few-Shot Learners [paper](https://arxiv.org/abs/2005.14165)
3. ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS [papers](https://arxiv.org/pdf/2003.10555.pdf)
4. SEQUENTIAL LATENT KNOWLEDGE SELECTION FOR KNOWLEDGE-GROUNDED DIALOGUE [papers](https://arxiv.org/pdf/2002.07510.pdf)
5. REFORMER: THE EFFICIENT TRANSFORMER [papers](https://arxiv.org/pdf/2001.04451.pdf)



## 김연수

- GPT3 : Language Models are Few-Shot Learners (https://arxiv.org/abs/2005.14165)
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (https://arxiv.org/abs/1910.13461)
- ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators (https://openreview.net/forum?id=r1xMH1BtvB)
- Big Bird: Transformers for Longer Sequences (https://arxiv.org/abs/2007.14062)
- T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (https://arxiv.org/abs/1910.10683)

## 류정현



## 백서인
1. ELMo: Deep contextualized word representations [paper](https://arxiv.org/pdf/1802.05365.pdf)
2. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations [paper](https://arxiv.org/pdf/1909.11942.pdf)
3. XLNet: Generalized Autoregressive Pretraining for Language Understanding [paper](https://arxiv.org/pdf/1906.08237.pdf)
4. GPT-2: Language Models are Unsupervised Multitask Learners [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
5. GPT-3: Language Models are Few-Shot Learners [paper](https://arxiv.org/pdf/2005.14165v4.pdf)


## 신한이


## 이유정
1. BART: https://arxiv.org/abs/1910.13461
2. XLNet : https://arxiv.org/abs/1906.08237 
3. ULMFit : https://arxiv.org/abs/1801.06146
4. GPT2 : https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
5. MT-DNN : https://arxiv.org/abs/1901.11504

## 전세희

1. XLNet : [link](https://arxiv.org/abs/1906.08237.pdf)
2. ALBERT : [link](https://arxiv.org/pdf/1909.11942.pdf)
3. ELMo : [link](https://arxiv.org/pdf/1802.05365.pdf)
4. Seq2seq : [link](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)
5. BART : [link](https://arxiv.org/pdf/1910.13461.pdf)



## 한지수
1. GPT-2: Language Models are Unsupervised Multitask Learners [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
2. GPT-3: Language Models are Few-Shot Learners [link](https://arxiv.org/pdf/2005.14165v4.pdf)
3. ELMo: Deep contextualized word representations [link](https://arxiv.org/pdf/1802.05365.pdf)
4. Big Bird: Transformers for Longer Sequences [link](https://arxiv.org/pdf/2007.14062.pdf)
5. mBART: Multilingual Denoising Pre-training for Neural Machine Translation [link](https://arxiv.org/pdf/2001.08210.pdf)


# Natural Language Processing Github 리스트(5개)



## 구재원
1. BERT [Link](https://github.com/google-research/bert)
2. ELMO [Link](https://github.com/allenai/bilm-tf)
3. GPT-2 [Link](ttps://github.com/graykode/gpt-2-Pytorch)
4. ALBERT [Link](https://github.com/google-research/albert)
5. Attention is all you need [Link](https://github.com/jadore801120/attention-is-all-you-need-pytorch)



## 김연수

1. BERT [Link](https://github.com/google-research/bert)
2. GPT2 [Link1-huggingface](https://github.com/huggingface/transformers/tree/master/src/transformers/models/gpt2) || [Link2](https://github.com/ConnorJL/GPT2) // GPT3는 비공식도 별로 없어서, 찾을 수 있다면 GPT3 하고싶네요
3. BART [Link-huggingface](https://github.com/huggingface/transformers/tree/1c06240e1b3477728129bb58e7b6c7734bb5074e/src/transformers/models/bart)
4. Transformer [Link1-huggingface](https://github.com/huggingface/transformers/tree/master/src/transformers) || [Link2](https://github.com/Kyubyong/transformer)
5. T5 [Link1](https://github.com/google-research/text-to-text-transfer-transformer) || [Link2-huggingface](https://github.com/huggingface/transformers/tree/master/src/transformers/models/t5)

## 류정현



## 백서인
1. ELMo: https://github.com/allenai/bilm-tf
2. ALBERT: https://github.com/google-research/albert
3. XLNet: https://github.com/zihangdai/xlnet
4. GPT-2: https://github.com/openai/gpt-2 , https://github.com/graykode/gpt-2-Pytorch 
5. GPT-3: https://github.com/openai/gpt-3 (코드는 없음)


## 신한이


## 이유정
1. BART: https://github.com/huggingface/transformers/tree/1c06240e1b3477728129bb58e7b6c7734bb5074e/src/transformers/models/bart
2. XLNet : https://github.com/zihangdai/xlnet
3. ULMFit : https://github.com/jannenev/ulmfit-language-model
4. GPT2 : https://github.com/graykode/gpt-2-Pytorch
5. MT-DNN : https://github.com/namisan/mt-dnn


## 전세희 (콜렉션 : [link](https://github.com/graykode/nlp-tutorial) )
1. BERT : 
- Google(Tensorflow) : [link](https://github.com/google-research/bert)
- Non official(PyTorch) : [link](https://github.com/codertimo/BERT-pytorch)
2. ELMo : [link](https://github.com/ratsgo/embedding) 저자 코드를 일부 수정한 모드
3. GPT-2: [link](https://github.com/openai/gpt-2)
4. Seq2seq : [link](https://github.com/graykode/nlp-tutorial) , (공식) [link](https://github.com/google/seq2seq)
5. ALBERT : [link](https://github.com/google-research/ALBERT)


## 한지수
1. GPT-2 [code](https://github.com/graykode/gpt-2-Pytorch )
2. GPT-3 [archived code](https://github.com/openai/gpt-3)
3. ELMo (공식코드 없음)
4. BigBird [code](https://github.com/google-research/bigbird)
5. mBART [huggingface code](https://github.com/huggingface/transformers)
